{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import date\n",
    "try:\n",
    "    from urlparse import urljoin  # Python2\n",
    "except ImportError:\n",
    "    from urllib.parse import urljoin  # Python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baixando sitemap.xml...\n",
      "sitemap.xml ok!\n"
     ]
    }
   ],
   "source": [
    "# Baixando o sitemap.xml do site\n",
    "print('baixando sitemap.xml...')\n",
    "url=\"https://www.foxterciaimobiliaria.com.br/sitemap.xml\"\n",
    "response = requests.get(url)\n",
    "with open('sitemap.xml', 'wb') as file:\n",
    "    file.write(response.content)\n",
    "print('sitemap.xml ok!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def union(tocrawl,newlinks):\n",
    "    for e in newlinks:\n",
    "        if e not in tocrawl:\n",
    "            tocrawl.append(e)\n",
    "    return   \n",
    "\n",
    "                  \n",
    "# Encontra todos os links na conteúdo Soup e guarda em alllinks\n",
    "def get_all_links(page):\n",
    "    links = []\n",
    "    for link in page_content(page).find_all('a'):\n",
    "        links.append(str(link.get('href')))\n",
    "    return links\n",
    "                  \n",
    "                  \n",
    "def check_links(alllinks):\n",
    "    links_checked = []\n",
    "\n",
    "    for link_to_check in alllinks:\n",
    "\n",
    "        if link_to_check[:38] == url_seed:\n",
    "            links_checked.append(link_to_check)\n",
    "        \n",
    "        if link_to_check[:8] == url_segment:\n",
    "            link_to_check_join = urljoin(url_seed, link_to_check)\n",
    "            links_checked.append(link_to_check_join)\n",
    "            \n",
    "    return links_checked    \n",
    "\n",
    "# Coletando conteúdo de uma página\n",
    "def page_content(page):\n",
    "\n",
    "    response = ''\n",
    "    while response == '':\n",
    "        wait = 0\n",
    "        try:\n",
    "            response = requests.get(page)\n",
    "        except:\n",
    "            wait = random.randint(10,2000) # caso o servidor não responda\n",
    "            print(\"Connection refused by the server..\")\n",
    "            print(\"Let me sleep for \" + str(wait) + \" seconds\")\n",
    "            print(\"ZZzzzz...\")\n",
    "            time.sleep(wait)\n",
    "            print(\"Was a nice sleep, now let me continue...\")\n",
    "            continue\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # ignorar se página sem id de imóvel\n",
    "    if soup.find('span', {'itemprop':\"identifier\"}) is None:\n",
    "        return soup\n",
    "    \n",
    "    #coletar id\n",
    "    id_imovel = soup.find('span', {'itemprop':\"identifier\"})\n",
    "    id_clean = id_imovel.get_text()[7:]\n",
    "    id_list.append(id_clean)\n",
    "        \n",
    "        \n",
    "    #coletar preço\n",
    "    try:        \n",
    "        price = soup.find('span', {'itemprop': \"price\"})\n",
    "        price_clean0 = price.get_text()[3:]\n",
    "        price_clean = price_clean0.replace(\".\", \"\")\n",
    "    except:\n",
    "        price_clean = 'NA'\n",
    "    price_list.append(price_clean)\n",
    "    \n",
    "        \n",
    "    #coletar área privativa\n",
    "    try:\n",
    "        area = soup.find(string='Área privativa').next_element.next_element\n",
    "        area_clean = area.get_text()[:-2]\n",
    "    except:\n",
    "        area_clean = 'NA'        \n",
    "    area_list.append(area_clean)\n",
    "        \n",
    "        \n",
    "    #coletar bairro\n",
    "    try:\n",
    "        district = soup.find(string='Bairro').next_element.next_element\n",
    "        district_clean = district.get_text()\n",
    "    except:\n",
    "        district_clean = 'NA'\n",
    "    district_list.append(district_clean)\n",
    "        \n",
    "        \n",
    "    #coletar cidade\n",
    "    try:\n",
    "        city = soup.find(string='Cidade').next_element.next_element\n",
    "        city_clean = city.get_text()\n",
    "    except:\n",
    "        city_clean = 'NA'\n",
    "    city_list.append(city_clean)\n",
    "\n",
    "        \n",
    "    #coletar tipo\n",
    "    try:\n",
    "        type1 = soup.find('span', {'itemprop': \"category\"})\n",
    "        type1_clean = type1.get_text()\n",
    "    except:\n",
    "        type1_clean = 'NA'\n",
    "    type1_list.append(type1_clean)\n",
    "\n",
    "\n",
    "    #coletar segmento\n",
    "    try:\n",
    "        segment = soup.find(string='Segmento').next_element.next_element\n",
    "        segment_clean = segment.get_text()\n",
    "    except:\n",
    "        segment_clean = 'NA'\n",
    "    segment_list.append(segment_clean)\n",
    "\n",
    "        \n",
    "    #coletar valor do condomínio\n",
    "    try:\n",
    "        condominium = soup.find(string='Condomínio').next_element.next_element\n",
    "        condominium_clean0 = condominium.get_text()[3:]\n",
    "        condominium_clean = condominium_clean0.replace(\".\", \"\")\n",
    "    except:\n",
    "        condominium_clean = 'NA'\n",
    "    condominium_list.append(condominium_clean)\n",
    "\n",
    "        \n",
    "    #coletar IPTU\n",
    "    try:\n",
    "        iptu = soup.find(string='IPTU Anual').next_element.next_element\n",
    "        iptu_clean0 = iptu.get_text()[3:]\n",
    "        iptu_clean = iptu_clean0.replace(\".\", \"\")\n",
    "    except:\n",
    "        iptu_clean = 'NA'\n",
    "    iptu_list.append(iptu_clean)\n",
    "    \n",
    "        \n",
    "    #coletar dormitórios\n",
    "    try:\n",
    "        rooms = soup.find(string='Dormitórios').next_element.next_element\n",
    "        rooms_clean = rooms.get_text()\n",
    "    except:\n",
    "        rooms_clean = 'NA'\n",
    "    rooms_list.append(rooms_clean)\n",
    "\n",
    "        \n",
    "    #coletar vagas para estacionamento\n",
    "    try:\n",
    "        box = soup.find('span', {'class': \"value vagas\"})\n",
    "        box_clean = box.get_text()\n",
    "    except:\n",
    "        box_clean = 'NA'\n",
    "    box_list.append(box_clean)\n",
    "                  \n",
    "        \n",
    "    #coletar url\n",
    "    url_list.append(page)\n",
    "    \n",
    "    #salvando data\n",
    "    date_list.append(date_today)\n",
    "    \n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando visita e captura\n",
    "def crawl_web(url_seed):\n",
    "    data_base = {}\n",
    "    tocrawl = [url_seed]\n",
    "    crawled = []\n",
    "    \n",
    "\n",
    "    # leitura do arquivo xml\n",
    "    with open('sitemap.xml') as file_object:\n",
    "        print('lendo sitemap.xml ...')\n",
    "        for line in file_object:\n",
    "            soup = BeautifulSoup(line, \"html.parser\")\n",
    "        \n",
    "            for link in soup.find_all('loc'):\n",
    "                tocrawl.append(str(link.get_text())) \n",
    "\n",
    "    print(\"leiutra do xml finalizada ... iniciando buscas...\\n\")\n",
    "    \n",
    "\n",
    "    while tocrawl:\n",
    "\n",
    "        page = tocrawl.pop()\n",
    "\n",
    "        sys.stdout.write(\"\\rpáginas analisadas: {}/{}.\".format(len(crawled), (len(crawled) + len(tocrawl))))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "        if page not in crawled:\n",
    "            \n",
    "            # segue se a página ainda não tiver sido coletada\n",
    "            \n",
    "            \n",
    "            # ignorar páginas que não sejam http\n",
    "            if page[:5] != \"http:\":\n",
    "                crawled.append(page)\n",
    "                continue\n",
    "            \n",
    "            # ignorar páginas de construtora\n",
    "            if \"/construtora\" in page:\n",
    "                crawled.append(page)\n",
    "                continue\n",
    "            \n",
    "            # ignorar páginas de empreendimento\n",
    "            if '/empreendimento' in page: \n",
    "                crawled.append(page)\n",
    "                continue\n",
    "            \n",
    "            # ignorar páginas de bairro\n",
    "            if '/bairro' in page: \n",
    "                crawled.append(page)\n",
    "                continue\n",
    "            \n",
    "            # ignorar páginas com erro de url\n",
    "            if '.com.brhttps:' in page: \n",
    "                crawled.append(page)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # delimitando a busca por (1) apartamento\n",
    "            # em (2) porto alegre\n",
    "            # (3) nos três bairros de interesse: auxiliadora, bela vista, mont serrat\n",
    "            if \"-porto-alegre-auxiliadora-apartamento-\" not in page:\n",
    "                if \"-porto-alegre-bela-vista-apartamento-\" not in page:\n",
    "                    if \"-porto-alegre-mont-serrat-apartamento-\" not in page:\n",
    "                        crawled.append(page)\n",
    "                        continue               \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            union(tocrawl, check_links(get_all_links(page)))\n",
    "            crawled.append(page)\n",
    "            \n",
    "            \n",
    "            #juntando as listas num dataframe\n",
    "            data_base = pd.DataFrame({'id': id_list,\n",
    "                                      'price': price_list,\n",
    "                                      'area': area_list,\n",
    "                                      'district': district_list,\n",
    "                                      'city': city_list,\n",
    "                                      'type': type1_list,\n",
    "                                      'segment': segment_list,\n",
    "                                      'condominium': condominium_list,\n",
    "                                      'iptu': iptu_list,\n",
    "                                      'rooms': rooms_list,\n",
    "                                      'box': box_list,\n",
    "                                      'url': url_list,\n",
    "                                      'date': date_list\n",
    "                                     })\n",
    "            \n",
    "            data_base.to_csv((date_today + '-foxter.csv'), sep='\\t') #salvando csv\n",
    "            \n",
    "\n",
    "    print(\"\\nfinalizado todo o site!!\")\n",
    "    print(\"\\ntotal de imóveis salvos: \" + str(data_base.shape[0]))\n",
    "\n",
    "    return\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lendo sitemap.xml ...\n",
      "leiutra do xml finalizada ... iniciando buscas...\n",
      "\n",
      "páginas analisadas: 27067/27067.\n",
      "finalizado todo o site!!\n",
      "\n",
      "total de imóveis salvos: 579\n"
     ]
    }
   ],
   "source": [
    "# inicialização do web scrapper\n",
    "date_today = str(date.today())\n",
    "id_list = []\n",
    "price_list = []\n",
    "area_list = []\n",
    "district_list = []\n",
    "city_list = []\n",
    "type1_list = []\n",
    "segment_list = []\n",
    "condominium_list = []\n",
    "iptu_list = []\n",
    "rooms_list = []\n",
    "box_list = []\n",
    "url_list = []\n",
    "date_list = []\n",
    "\n",
    "\n",
    "\n",
    "url_seed = (\"http://www.foxterciaimobiliaria.com.br\") # Página inicial - semente\n",
    "\n",
    "url_segment = '/imovel/' # identificação de segmento de produto no site\n",
    "\n",
    "\n",
    "\n",
    "crawl_web(url_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
